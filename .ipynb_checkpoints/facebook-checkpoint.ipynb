{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data Analytics of Facebook Comments\n",
    "\n",
    "#### Description: \n",
    "Our project aims at classifying Facebook comments from business pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Import and Preprocessing\n",
    "\n",
    "#### 1.1 Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build spark session and spark context\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"fb\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='634995', _c1='0', _c2='463', _c3='1', _c4='0.0', _c5='806.0', _c6='11.291044776119403', _c7='1.0', _c8='70.49513846124168', _c9='0.0', _c10='806.0', _c11='7.574626865671642', _c12='0.0', _c13='69.435826365571', _c14='0.0', _c15='76.0', _c16='2.6044776119402986', _c17='0.0', _c18='8.50550186882253', _c19='0.0', _c20='806.0', _c21='10.649253731343284', _c22='1.0', _c23='70.25478763764251', _c24='-69.0', _c25='806.0', _c26='4.970149253731344', _c27='0.0', _c28='69.85058043098057', _c29='0', _c30='0', _c31='0', _c32='0', _c33='0', _c34='65', _c35='166', _c36='2', _c37='0', _c38='24', _c39='0', _c40='0', _c41='0', _c42='1', _c43='0', _c44='0', _c45='0', _c46='0', _c47='0', _c48='0', _c49='0', _c50='0', _c51='0', _c52='1', _c53='0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fb = spark.read.csv('Features_Variant_1.csv')\n",
    "\n",
    "fb=spark.read.csv('/home/jovyan/Project/Training/Features_Variant_1.csv')\n",
    "fb.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rename Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name dataframe with correct column names as \"df\"\n",
    "df = (fb.withColumnRenamed('_c0', 'PageLikes')              # page likes\n",
    "             .withColumnRenamed('_c1', 'PageCheckin')       # page's \"Check-in\" count\n",
    "             .withColumnRenamed('_c2', 'PageTalking')       # page's \"Talking About\" count\n",
    "             .withColumnRenamed('_c3', 'PageCategory')      # page category\n",
    "             \n",
    "            # C1: total comment count before selected base date/time\n",
    "             .withColumnRenamed('_c4', 'C1min')             # min of C1\n",
    "             .withColumnRenamed('_c5', 'C1max')             # max of C1\n",
    "             .withColumnRenamed('_c6', 'C1avg')             # avg of C1\n",
    "             .withColumnRenamed('_c7', 'C1med')             # median of C1\n",
    "             .withColumnRenamed('_c8', 'C1std')             # standard deviation of C1\n",
    "      \n",
    "            # C2: comment count in last 24 hrs w.r.t base date/time\n",
    "             .withColumnRenamed('_c9', 'C2min')             # min of C2\n",
    "             .withColumnRenamed('_c10', 'C2max')            # max of C2\n",
    "             .withColumnRenamed('_c11', 'C2avg')            # avg of C2\n",
    "             .withColumnRenamed('_c12', 'C2med')            # median of C2\n",
    "             .withColumnRenamed('_c13', 'C2std')            # standard deviation of C2\n",
    "    \n",
    "            # C3: comment count from last 48 to last 24 hrs w.r.t base date/time\n",
    "             .withColumnRenamed('_c14', 'C3min')            # min of C3\n",
    "             .withColumnRenamed('_c15', 'C3max')            # max of C3\n",
    "             .withColumnRenamed('_c16', 'C3avg')            # avg of C3\n",
    "             .withColumnRenamed('_c17', 'C3med')            # median of C3\n",
    "             .withColumnRenamed('_c18', 'C3std')            # standard deviation of C3\n",
    "\n",
    "            # C4: comment count in first 24 hrs after publishing document but before base date/time\n",
    "             .withColumnRenamed('_c19', 'C4min')            # min of C4\n",
    "             .withColumnRenamed('_c20', 'C4max')            # max of C4\n",
    "             .withColumnRenamed('_c21', 'C4avg')            # avg of C4\n",
    "             .withColumnRenamed('_c22', 'C4med')            # median of C4\n",
    "             .withColumnRenamed('_c23', 'C4std')            # standard deviation of C4\n",
    "      \n",
    "            # C5: difference between C2 and C3\n",
    "             .withColumnRenamed('_c24', 'C5min')            # min of C5\n",
    "             .withColumnRenamed('_c25', 'C5max')            # max of C5\n",
    "             .withColumnRenamed('_c26', 'C5avg')            # avg of C5\n",
    "             .withColumnRenamed('_c27', 'C5med')            # median of C5\n",
    "             .withColumnRenamed('_c28', 'C5std')            # standard deviation of C5\n",
    "      \n",
    "            # Other features\n",
    "             .withColumnRenamed('_c29', 'CC1')              # total number of comments before selected base date/time\n",
    "             .withColumnRenamed('_c30', 'CC2')              # number of comments in last 24 hours relative to base date/time\n",
    "             .withColumnRenamed('_c31', 'CC3')              # number of comments in last 48 hours to last 24 hours relative to base date/time\n",
    "             .withColumnRenamed('_c32', 'CC4')              # number of comments in the first 24 hours after the publication of post but before base date/time\n",
    "             .withColumnRenamed('_c33', 'CC5')              # difference between CC2 and CC3\n",
    "             .withColumnRenamed('_c34', 'BaseTime')         # selected time in order to simulate scenario (this is a decimal between 0-71, not sure what the number specifically means)\n",
    "             .withColumnRenamed('_c35', 'PostLength')       # character count in post\n",
    "             .withColumnRenamed('_c36', 'PostShare')        # number of shares of the post, how many people shared this post to their timeline\n",
    "             .withColumnRenamed('_c37', 'PagePromote')      # if the original poster \"promoted\" this post\n",
    "             .withColumnRenamed('_c38', 'HLocal')           # describes the hours past for target variable\n",
    "             \n",
    "            # Binary variables that represent the day(Sun-Sat) on which the post was published\n",
    "             .withColumnRenamed('_c39', 'PostSun')         \n",
    "             .withColumnRenamed('_c40', 'PostMon')          \n",
    "             .withColumnRenamed('_c41', 'PostTue') \n",
    "             .withColumnRenamed('_c42', 'PostWed') \n",
    "             .withColumnRenamed('_c43', 'PostThu') \n",
    "             .withColumnRenamed('_c44', 'PostFri')   \n",
    "             .withColumnRenamed('_c45', 'PostSat') \n",
    "      \n",
    "            # Binary variables that represent the day(Sun-Sat) of selected base Date/Time\n",
    "             .withColumnRenamed('_c46', 'BaseSun')          \n",
    "             .withColumnRenamed('_c47', 'BaseMon') \n",
    "             .withColumnRenamed('_c48', 'BaseTue')   \n",
    "             .withColumnRenamed('_c49', 'BaseWed') \n",
    "             .withColumnRenamed('_c50', 'BaseThu')   \n",
    "             .withColumnRenamed('_c51', 'BaseFri')\n",
    "             .withColumnRenamed('_c52', 'BaseSat') \n",
    "      \n",
    "            # Target Variable: number of comments in next H hrs after comment was posted\n",
    "             .withColumnRenamed('_c53', 'Comments'))       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Casting each variable to appropriate datatype.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataframe named \"df_casted\"\n",
    "df_casted = df.select(\n",
    "                df.PageLikes.cast('float'),\n",
    "                df.PageCheckin.cast('float'),\n",
    "                df.PageTalking.cast('float'),\n",
    "                df.PageCategory.cast('int'),  # number corresponds to categories in pdf\n",
    "                df.C1min.cast('float'),\n",
    "                df.C1max.cast('float'),\n",
    "                df.C1avg.cast('float'),\n",
    "                df.C1med.cast('float'),\n",
    "                df.C1std.cast('float'),\n",
    "                df.C2min.cast('float'),\n",
    "                df.C2max.cast('float'),\n",
    "                df.C2avg.cast('float'),\n",
    "                df.C2med.cast('float'),\n",
    "                df.C2std.cast('float'),\n",
    "                df.C3min.cast('float'),\n",
    "                df.C3max.cast('float'),\n",
    "                df.C3avg.cast('float'),\n",
    "                df.C3med.cast('float'),\n",
    "                df.C3std.cast('float'),\n",
    "                df.C4min.cast('float'),\n",
    "                df.C4max.cast('float'),\n",
    "                df.C4avg.cast('float'),\n",
    "                df.C4med.cast('float'),\n",
    "                df.C4std.cast('float'),\n",
    "                df.C5min.cast('float'),\n",
    "                df.C5max.cast('float'),\n",
    "                df.C5avg.cast('float'),\n",
    "                df.C5med.cast('float'),\n",
    "                df.C5std.cast('float'),\n",
    "                df.CC1.cast('float'),\n",
    "                df.CC2.cast('float'),\n",
    "                df.CC3.cast('float'),\n",
    "                df.CC4.cast('float'),\n",
    "                df.CC5.cast('float'),\n",
    "                df.BaseTime.cast('float'),\n",
    "                df.PostLength.cast('float'),\n",
    "                df.PostShare.cast('float'),\n",
    "                df.PagePromote.cast('int'),  # integer encoded\n",
    "                df.HLocal.cast('float'),\n",
    "                df.PostSun.cast('int'),\n",
    "                df.PostMon.cast('int'),\n",
    "                df.PostTue.cast('int'),\n",
    "                df.PostWed.cast('int'),\n",
    "                df.PostThu.cast('int'),\n",
    "                df.PostFri.cast('int'),\n",
    "                df.PostSat.cast('int'),\n",
    "                df.BaseSun.cast('int'),\n",
    "                df.BaseMon.cast('int'),\n",
    "                df.BaseTue.cast('int'),\n",
    "                df.BaseThu.cast('int'),\n",
    "                df.BaseFri.cast('int'),\n",
    "                df.BaseSat.cast('int'),\n",
    "                df.Comments.cast('float')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PageLikes: float (nullable = true)\n",
      " |-- PageCheckin: float (nullable = true)\n",
      " |-- PageTalking: float (nullable = true)\n",
      " |-- PageCategory: integer (nullable = true)\n",
      " |-- C1min: float (nullable = true)\n",
      " |-- C1max: float (nullable = true)\n",
      " |-- C1avg: float (nullable = true)\n",
      " |-- C1med: float (nullable = true)\n",
      " |-- C1std: float (nullable = true)\n",
      " |-- C2min: float (nullable = true)\n",
      " |-- C2max: float (nullable = true)\n",
      " |-- C2avg: float (nullable = true)\n",
      " |-- C2med: float (nullable = true)\n",
      " |-- C2std: float (nullable = true)\n",
      " |-- C3min: float (nullable = true)\n",
      " |-- C3max: float (nullable = true)\n",
      " |-- C3avg: float (nullable = true)\n",
      " |-- C3med: float (nullable = true)\n",
      " |-- C3std: float (nullable = true)\n",
      " |-- C4min: float (nullable = true)\n",
      " |-- C4max: float (nullable = true)\n",
      " |-- C4avg: float (nullable = true)\n",
      " |-- C4med: float (nullable = true)\n",
      " |-- C4std: float (nullable = true)\n",
      " |-- C5min: float (nullable = true)\n",
      " |-- C5max: float (nullable = true)\n",
      " |-- C5avg: float (nullable = true)\n",
      " |-- C5med: float (nullable = true)\n",
      " |-- C5std: float (nullable = true)\n",
      " |-- CC1: float (nullable = true)\n",
      " |-- CC2: float (nullable = true)\n",
      " |-- CC3: float (nullable = true)\n",
      " |-- CC4: float (nullable = true)\n",
      " |-- CC5: float (nullable = true)\n",
      " |-- BaseTime: float (nullable = true)\n",
      " |-- PostLength: float (nullable = true)\n",
      " |-- PostShare: float (nullable = true)\n",
      " |-- PagePromote: integer (nullable = true)\n",
      " |-- HLocal: float (nullable = true)\n",
      " |-- PostSun: integer (nullable = true)\n",
      " |-- PostMon: integer (nullable = true)\n",
      " |-- PostTue: integer (nullable = true)\n",
      " |-- PostWed: integer (nullable = true)\n",
      " |-- PostThu: integer (nullable = true)\n",
      " |-- PostFri: integer (nullable = true)\n",
      " |-- PostSat: integer (nullable = true)\n",
      " |-- BaseSun: integer (nullable = true)\n",
      " |-- BaseMon: integer (nullable = true)\n",
      " |-- BaseTue: integer (nullable = true)\n",
      " |-- BaseThu: integer (nullable = true)\n",
      " |-- BaseFri: integer (nullable = true)\n",
      " |-- BaseSat: integer (nullable = true)\n",
      " |-- Comments: float (nullable = true)\n",
      " |-- PageCategory_onehot: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check schema\n",
    "df_casted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummy Variable Construction**\n",
    "this is basically transforming categorical variables into separate, binary-encoded variables. For ex: if variable is DayofWeek and levels are Sun-Sat, then we would create 7 new variables each named Sun-Sat with values of 0 or 1 (0 if not that day 1 if it is). This allows us to input those variables into the regression model, because otherwise a regression model can not process categorical variables. \n",
    "\n",
    "Luckily for us, days of the week are already in a dummy-variable format. The only other variable I see that is categorical is *Page Category*, which is numerically encoded with each number representing a different category. (You can view the different categories in the pdf called PageCategories located on the Github) I think dummy variables will not be realistic as there are too many levels of category, so maybe we can see if category makes a large difference through graphs and omit it from the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+------------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+-----+-----+---------+-----+--------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+---+---+---+---+----+--------+----------+---------+-----------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "|PageLikes|PageCheckin|PageTalking|PageCategory|C1min|C1max|    C1avg|C1med|   C1std|C2min|C2max|   C2avg|C2med|   C2std|C3min|C3max|    C3avg|C3med|   C3std|C4min|C4max|    C4avg|C4med|   C4std|C5min|C5max|   C5avg|C5med|   C5std|CC1|CC2|CC3|CC4| CC5|BaseTime|PostLength|PostShare|PagePromote|HLocal|PostSun|PostMon|PostTue|PostWed|PostThu|PostFri|PostSat|BaseSun|BaseMon|BaseTue|BaseThu|BaseFri|BaseSat|Comments|\n",
      "+---------+-----------+-----------+------------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+-----+-----+---------+-----+--------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+---+---+---+---+----+--------+----------+---------+-----------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "| 634995.0|        0.0|      463.0|           1|  0.0|806.0|11.291045|  1.0|70.49514|  0.0|806.0|7.574627|  0.0|69.43583|  0.0| 76.0|2.6044776|  0.0|8.505502|  0.0|806.0|10.649254|  1.0|70.25479|-69.0|806.0|4.970149|  0.0|69.85058|0.0|0.0|0.0|0.0| 0.0|    65.0|     166.0|      2.0|          0|  24.0|      0|      0|      0|      1|      0|      0|      0|      0|      0|      0|      0|      0|      1|     0.0|\n",
      "| 634995.0|        0.0|      463.0|           1|  0.0|806.0|11.291045|  1.0|70.49514|  0.0|806.0|7.574627|  0.0|69.43583|  0.0| 76.0|2.6044776|  0.0|8.505502|  0.0|806.0|10.649254|  1.0|70.25479|-69.0|806.0|4.970149|  0.0|69.85058|0.0|0.0|0.0|0.0| 0.0|    10.0|     132.0|      1.0|          0|  24.0|      0|      0|      0|      0|      1|      0|      0|      0|      0|      0|      0|      1|      0|     0.0|\n",
      "| 634995.0|        0.0|      463.0|           1|  0.0|806.0|11.291045|  1.0|70.49514|  0.0|806.0|7.574627|  0.0|69.43583|  0.0| 76.0|2.6044776|  0.0|8.505502|  0.0|806.0|10.649254|  1.0|70.25479|-69.0|806.0|4.970149|  0.0|69.85058|0.0|0.0|0.0|0.0| 0.0|    14.0|     133.0|      2.0|          0|  24.0|      0|      0|      0|      0|      0|      1|      0|      0|      0|      0|      0|      0|      1|     0.0|\n",
      "| 634995.0|        0.0|      463.0|           1|  0.0|806.0|11.291045|  1.0|70.49514|  0.0|806.0|7.574627|  0.0|69.43583|  0.0| 76.0|2.6044776|  0.0|8.505502|  0.0|806.0|10.649254|  1.0|70.25479|-69.0|806.0|4.970149|  0.0|69.85058|7.0|0.0|3.0|7.0|-3.0|    62.0|     131.0|      1.0|          0|  24.0|      0|      0|      0|      0|      0|      1|      0|      0|      1|      0|      0|      0|      0|     0.0|\n",
      "| 634995.0|        0.0|      463.0|           1|  0.0|806.0|11.291045|  1.0|70.49514|  0.0|806.0|7.574627|  0.0|69.43583|  0.0| 76.0|2.6044776|  0.0|8.505502|  0.0|806.0|10.649254|  1.0|70.25479|-69.0|806.0|4.970149|  0.0|69.85058|1.0|0.0|0.0|1.0| 0.0|    58.0|     142.0|      5.0|          0|  24.0|      0|      1|      0|      0|      0|      0|      0|      0|      0|      0|      0|      0|      0|     0.0|\n",
      "+---------+-----------+-----------+------------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+-----+-----+---------+-----+--------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+---+---+---+---+----+--------+----------+---------+-----------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show top 5 rows\n",
    "df_casted.select('*').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+------------+\n",
      "|PageLikes|PageCheckin|PageTalking|PageCategory|\n",
      "+---------+-----------+-----------+------------+\n",
      "| 634995.0|        0.0|      463.0|           1|\n",
      "| 634995.0|        0.0|      463.0|           1|\n",
      "| 634995.0|        0.0|      463.0|           1|\n",
      "| 634995.0|        0.0|      463.0|           1|\n",
      "| 634995.0|        0.0|      463.0|           1|\n",
      "+---------+-----------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show top 5 rows of Page Demographics\n",
    "page_demo_col = ['PageLikes', 'PageCheckin', 'PageTalking', 'PageCategory']\n",
    "page_demo = [x for x in df_casted.columns if x in page_demo_col]\n",
    "df_casted.select(page_demo).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|PageCategory|count|\n",
      "+------------+-----+\n",
      "|           9| 7494|\n",
      "|          24| 4511|\n",
      "|          18| 4301|\n",
      "|          36| 2387|\n",
      "|          16| 1890|\n",
      "|          14| 1684|\n",
      "|           8| 1446|\n",
      "|          32| 1388|\n",
      "|           4| 1110|\n",
      "|           2| 1050|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the different Page Categories\n",
    "#Order by descending counts - top 10 types of categories\n",
    "from pyspark.sql.functions import col, desc\n",
    "df_casted.groupBy('PageCategory').count().orderBy(col('count').desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|PageCategory|count|\n",
      "+------------+-----+\n",
      "|          93|    1|\n",
      "|          83|    1|\n",
      "|          58|    2|\n",
      "|          63|    4|\n",
      "|          62|   16|\n",
      "|          33|   17|\n",
      "|          11|   19|\n",
      "|          81|   22|\n",
      "|          89|   22|\n",
      "|          82|   24|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Least types of categories - bottom 10\n",
    "df_casted.groupBy('PageCategory').count().orderBy(col('count'), ascending=True).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that different categories may be more represented than others. What we can do is filter and/or group our PageCategory into separate clusters so that one cluster represents page categories that are well represnted (top 10), and page categories that are least represented (bottom 10). We can then look into the features of the pages (e.g., number of likes to a certain time/day), and assess rate of increases.\n",
    "\n",
    "Other questions: do more characters (character count) in a post by a page receive more comments and likes than posts that say or are written with less words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Index 1 is 'Product/service'\n",
    "df_casted.filter(df_casted.PageCategory == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Index 3 is 'Education website'\n",
    "df_casted.filter(df_casted.PageCategory == 3).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Index 7 is 'Professional sports team'\n",
    "df_casted.filter(df_casted.PageCategory == 7).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, OneHotEncoderModel\n",
    "\n",
    "ohe1 = OneHotEncoder()\n",
    "ohe1.setInputCols([\"PageCategory\"])\n",
    "ohe1.setOutputCols([\"PageCategory_onehot\"])\n",
    "model1 = ohe1.fit(df_casted)\n",
    "model1.setOutputCols([\"PageCategory_onehot\"])\n",
    "df_casted=model1.transform(df_casted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+------------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+-----+-----+---------+-----+--------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+---+---+---+---+---+--------+----------+---------+-----------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+-------------------+\n",
      "|PageLikes|PageCheckin|PageTalking|PageCategory|C1min|C1max|    C1avg|C1med|   C1std|C2min|C2max|   C2avg|C2med|   C2std|C3min|C3max|    C3avg|C3med|   C3std|C4min|C4max|    C4avg|C4med|   C4std|C5min|C5max|   C5avg|C5med|   C5std|CC1|CC2|CC3|CC4|CC5|BaseTime|PostLength|PostShare|PagePromote|HLocal|PostSun|PostMon|PostTue|PostWed|PostThu|PostFri|PostSat|BaseSun|BaseMon|BaseTue|BaseThu|BaseFri|BaseSat|Comments|PageCategory_onehot|\n",
      "+---------+-----------+-----------+------------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+-----+-----+---------+-----+--------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+---+---+---+---+---+--------+----------+---------+-----------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+-------------------+\n",
      "| 634995.0|        0.0|      463.0|           1|  0.0|806.0|11.291045|  1.0|70.49514|  0.0|806.0|7.574627|  0.0|69.43583|  0.0| 76.0|2.6044776|  0.0|8.505502|  0.0|806.0|10.649254|  1.0|70.25479|-69.0|806.0|4.970149|  0.0|69.85058|0.0|0.0|0.0|0.0|0.0|    65.0|     166.0|      2.0|          0|  24.0|      0|      0|      0|      1|      0|      0|      0|      0|      0|      0|      0|      0|      1|     0.0|    (106,[1],[1.0])|\n",
      "+---------+-----------+-----------+------------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+-----+-----+---------+-----+--------+-----+-----+---------+-----+--------+-----+-----+--------+-----+--------+---+---+---+---+---+--------+----------+---------+-----------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_casted.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate 'PostSun' to 'PostSat' into one columsn named 'PostDay' so that the column is a string that is 'Sunday', 'Monday', Tuesday'..., 'Saturday' if a '1' was labeled for the column in 'PostSun':'PostSat'. This will remove unncessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PostLength', 'PostShare', 'PostSun', 'PostMon', 'PostTue', 'PostWed', 'PostThu', 'PostFri', 'PostSat']\n"
     ]
    }
   ],
   "source": [
    "#Clean up date of publishing: Sun-Sat columns (yes/no)\n",
    "#maybe use FILTER function?\n",
    "page_and_day = [col for col in df_casted.columns if 'Post' in col]\n",
    "print(page_and_day)\n",
    "#df_casted.select(page_demo).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling missing values and outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What variables are we assessing for missing values?\n",
    "#change 'ratings' to relevant variable\n",
    "df_noNULL = df_casted.filter(df_casted.ratings.isNotNull())\n",
    "df_noNULL.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate no nulls to check \n",
    "#change variables \n",
    "df_noNULL.groupBy(df_noNULL['id'])\\\n",
    "                  .agg({'ratings': 'mean'})\\\n",
    "                  .orderBy('id').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling semi structured/unstructured data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimension reduction (like PCA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA code to modify\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "rows = sc.parallelize([\n",
    "    Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "    Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "    Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "])\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "#Computer the top 4 PCs, which are stored in a local dense matrix\n",
    "pc = mat.computePrincipalComponents(5)\n",
    "\n",
    "#Project the rows to the linear space spanned by the top 4 PCs\n",
    "projected = mat.multiply(pc)\n",
    "\n",
    "#Collect and print results\n",
    "collected = projected.rows.collect()\n",
    "print(\"Projected Row Matrix of principal components:\")\n",
    "\n",
    "p = np.vstack((collected[0].toArray(), collected[1].toArray(), collected[2].toArray()))\n",
    "var_ratio = np.var(p, axis=0)/sum(np.var(p, axis=0))\n",
    "pc_values = np.arange(5) + 1\n",
    "plt.plot(pc_values, var_ratio)\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Proportion of Explained Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVD code to modify\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "rows = sc.parallelize([\n",
    "    Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "    Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "    Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "])\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Compute the top k singular values and corresponding singular vectors.\n",
    "topk = 4 #the rank you want\n",
    "svd = mat.computeSVD(topk, computeU=True)\n",
    "#The following U, s, and V are approximations because we use k=4 when we have 5 diemnsional matrix\n",
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix.\n",
    "\n",
    "collected = U.rows.collect()\n",
    "print(\"U factor is:\")\n",
    "for vector in collected:\n",
    "    print(vector)\n",
    "print(\"Singular values are: %s\" % s)\n",
    "print(\"V factor is:\\n%s\" % V)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Splitting and Sampling\n",
    "- Split into train/test sets. Try 60/40 and 70/30 splits. We may have to do k-fold cross validation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis (at least 2 graphs)\n",
    "- Consider histograms, bar plots, box plots, etc \n",
    "- add colors/legends to separate groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Construction (at least 3 models)\n",
    "Remember to: Construct each model using pipelines\n",
    "- Create a regression model as a benchmark model, simple with small number of features. We will use this as a basis of comparison.\n",
    "- Create 1 or 2 more sophisticated models, look amongst the ones covered in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation\n",
    "\n",
    "**Metrics for Regression:**\n",
    "- R-squared (for single factor)\n",
    "- Adjusted R-squared (for multifactor)\n",
    "\n",
    "**Metrics for Classification:**\n",
    "- accuracy\n",
    "- precision, recall, F1 score\n",
    "- confusion matrix\n",
    "- area under ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Outside Resources:\n",
    "- paper explaining some of the variables on pg 16.3: [link](https://ijssst.info/Vol-16/No-5/paper16.pdf)\n",
    "- assignment i found online based on this dataset, might give us some good questions to ask: [link](http://cse.ucdenver.edu/~biswasa/ml-s18/files/assignments/assignment-01/Programming-Assignment-1.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
