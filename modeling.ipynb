{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build spark session and spark context\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"hotel\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df = spark.read.csv('hotel_bookings.csv',  inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# replace the strings \"NULL\" and \"NA\" with null value\n",
    "df_withNull = df.replace('NULL', None).replace('NA', None)\n",
    "\n",
    "# replace null values in 'children' to 0 since there are only 4\n",
    "df2 = df_withNull.fillna({'children':0})\n",
    "\n",
    "# replace 'children' datatype to int\n",
    "df2 = df2.withColumn('children', col('children').cast(\"Int\"))\n",
    "\n",
    "# drop 'company' and 'agent' due to high null count\n",
    "df2 = df2.drop('agent', 'company','country', 'arrival_date_week_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addressing reservation_status_date, which gives the date at which the last \n",
    "# reservation status was set. I'm transforming it into number of days since reservation\n",
    "# status was set, so day of arrival - reservation_status_date\n",
    "# from pyspark.sql.types import DateType\n",
    "\n",
    "# convert reservation_status_date into datetype dtype\n",
    "# temp = df2.withColumn(\"reservation_status_date\", df2[\"reservation_status_date\"].cast(DateType()))\n",
    "\n",
    "# need to combine arrival_date_year, arrival_date_month, and arrival_date_day_of_month\n",
    "# into one column and cast it to DateType, then replacing reservation_status_date column\n",
    "# with number of days since reservation.\n",
    "# until then, dropping reservation_status_date\n",
    "df2 = df2 .drop('reservation_status_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerically encode all columns of type string\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, OneHotEncoderModel, StringIndexer\n",
    "\n",
    "# list of columns to numerically encode\n",
    "col_string=['hotel', 'meal','market_segment',\n",
    "            'distribution_channel','reserved_room_type','assigned_room_type',\n",
    "            'deposit_type','customer_type','reservation_status']\n",
    "col_stringwmonth = col_string+['arrival_date_month']\n",
    "\n",
    "\n",
    "# col_num = list of new column names being changed into numeric\n",
    "col_num=[x+\"_NUMERIC\" for x in col_string]\n",
    "# don't need to change dates into numeric col_num=col_num+['arrival_date_year','arrival_date_day_of_month']\n",
    "\n",
    "# col_oh = list of columns being one-hot encoded\n",
    "col_oh=[x+\"_oh\" for x in col_string]\n",
    "# dont need to ohencode dates col_oh=col_oh+['arrival_date_year_oh','arrival_date_day_of_month_oh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_NUMERIC\").fit(df2) for column in col_stringwmonth]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_indexed = pipeline.fit(df2).transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode all columns in col_num\n",
    "ohe = OneHotEncoder(dropLast=False)\n",
    "ohe.setInputCols(col_num)\n",
    "ohe.setOutputCols(col_oh)\n",
    "model = ohe.fit(df_indexed)\n",
    "\n",
    "df_casted=model.transform(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original, non-ohencoded variables\n",
    "df_encoded=df_casted.drop(*col_stringwmonth)\n",
    "df_encoded=df_encoded.drop(*col_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quick summary to explain below: we start with df_encoded, which is dataframe with all variables numerically encoded, with categorical variables one hot encoded. convert to rdd to so VectorAssembler can combine all features into one vector named \"features\". then convert back to dataframe to normalize all features, which should only change the continuous variables as normalizing on categorical does nothing. Then, convert the normalized dataframe back to rdd in order to feed it into LabelEncoder. Finally, the \"lp\" is the labeledpoint object that holds two columns, labels and features. from here use lp for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# combine all feature columns (non-label columns) into one vector named \"feature\"\n",
    "assembler = VectorAssembler(inputCols=df_encoded.columns[1:], outputCol=\"features\")\n",
    "transformed = assembler.transform(df_encoded)\n",
    "\n",
    "# conver to rdd with 2 columns, label and features, where features is a DenseVector combination of all other features\n",
    "dataRdd = transformed.select(col(\"is_canceled\").alias(\"label\"), col(\"features\")).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|        normFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|(69,[0,1,2,5,11,1...|(69,[0,1,2,5,11,1...|\n",
      "|    0|(69,[0,1,2,5,11,1...|(69,[0,1,2,5,11,1...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    0|(69,[1,2,4,5,13,1...|(69,[1,2,4,5,13,1...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    1|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    1|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    1|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    0|(69,[0,1,2,4,5,6,...|(69,[0,1,2,4,5,6,...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "|    0|(69,[1,2,4,5,13,1...|(69,[1,2,4,5,13,1...|\n",
      "|    0|(69,[0,1,2,4,5,13...|(69,[0,1,2,4,5,13...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# convert rdd to dataframe again in order to normalize on features column\n",
    "df_transformed = dataRdd.toDF(['label', 'features'])\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(df_transformed)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()\n",
    "\n",
    "normRdd = l1NormData.select(col(\"label\"), col(\"normFeatures\")).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [342.0,2015.0,1.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,3.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# map features as floats, then input all into a DenseVector, as well as mapping labels as floats\n",
    "# then map into LabeledPoint object\n",
    "lp = normRdd.map(lambda row: (float(row[0]), Vectors.dense([float(c) for c in row[1]])))\\\n",
    "            .map(lambda row: LabeledPoint(row[0], row[1]))\n",
    "lp.take(1)\n",
    "# not sure why the labeledpoint looks like it has so many 0's \n",
    "# as the features/normFeatures above looks like it has the correct values. need to investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83649"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 70/30 test train split\n",
    "seed = 42\n",
    "train, test = lp.randomSplit([0.7, 0.3], seed)\n",
    "\n",
    "# count number of rows in train, but it takes a while\n",
    "# train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# train logistic regression model with training data\n",
    "model = LogisticRegressionWithLBFGS.train(train)\n",
    "\n",
    "# evaluating model on training data\n",
    "labelsAndPreds = train.map(lambda p:(p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(train.count(()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
