{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build spark session and spark context\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"hotel\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df = spark.read.csv('hotel_bookings.csv',  inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|is_canceled|count|\n",
      "+-----------+-----+\n",
      "|          1|44224|\n",
      "|          0|75166|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('is_canceled').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for defining code below\n",
    "target = 'is_canceled'\n",
    "cancel_label = 1\n",
    "noncancel_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balancing a DataFrame with Downsampling\n",
    "\n",
    "def downsample(df, target, cancel_label, noncancel_label):\n",
    "    \"\"\"\n",
    "    df               spark dataframe\n",
    "    target           str, target variable\n",
    "    cancel_label     int, value of canceled booking\n",
    "    noncancel_label  int, value of non-canceled booking\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ### ENTER CODE HERE\n",
    "    \n",
    "    from pyspark.sql.functions import col\n",
    "    \n",
    "    #count of canceled and non-canceled labels\n",
    "    cancel_n = df.filter(col(target) == cancel_label).count()\n",
    "    noncancel_n = df.filter(col(target) == noncancel_label).count()\n",
    "    \n",
    "    #df split by having either the poitive or negative labels\n",
    "    df_cancel = df.filter(col(target) == cancel_label)\n",
    "    df_noncancel = df.filter(col(target) == noncancel_label)\n",
    "    \n",
    "    \n",
    "    if cancel_n > noncancel_n:\n",
    "        #amount to sample from is fraction of low noncancel/full cancel\n",
    "        df_a = df_cancel.sample(fraction = (noncancel_n/cancel_n))\n",
    "        #combine df_cancel sample with full df_noncancel\n",
    "        df_b = df_noncancel.union(df_a)\n",
    "    elif noncancel_n > cancel_n:\n",
    "        #amount to sample from is fraction of low cancel/full non-cancel\n",
    "        df_a = df_noncancel.sample(fraction = (cancel_n/noncancel_n))\n",
    "        #combine df_noncancel sample with full df_cancel\n",
    "        df_b = df_cancel.union(df_a)\n",
    "    else:\n",
    "        #if count of df_cancel = df_noncancel, then just use original df\n",
    "        df_b = df\n",
    "\n",
    "    return df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|is_canceled|count|\n",
      "+-----------+-----+\n",
      "|          1|44224|\n",
      "|          0|44296|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call your downsample function here, and show the count by label\n",
    "df_downsample = downsample(df, target, cancel_label, noncancel_label)\n",
    "df_downsample.groupBy(target).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# replace the strings \"NULL\" and \"NA\" with null value\n",
    "df_withNull = df_downsample.replace('NULL', None).replace('NA', None)\n",
    "#df_withNull = df.replace('NULL', None).replace('NA', None)\n",
    "\n",
    "# replace null values in 'children' to 0 since there are only 4\n",
    "df2 = df_withNull.fillna({'children':0})\n",
    "\n",
    "# replace 'children' datatype to int\n",
    "df2 = df2.withColumn('children', col('children').cast(\"Int\"))\n",
    "\n",
    "# drop 'company' and 'agent' due to high null count\n",
    "df2 = df2.drop('agent', 'company','country', 'arrival_date_week_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addressing reservation_status_date, which gives the date at which the last \n",
    "# reservation status was set. I'm transforming it into number of days since reservation\n",
    "# status was set, so day of arrival - reservation_status_date\n",
    "# from pyspark.sql.types import DateType\n",
    "\n",
    "# convert reservation_status_date into datetype dtype\n",
    "# temp = df2.withColumn(\"reservation_status_date\", df2[\"reservation_status_date\"].cast(DateType()))\n",
    "\n",
    "# need to combine arrival_date_year, arrival_date_month, and arrival_date_day_of_month\n",
    "# into one column and cast it to DateType, then replacing reservation_status_date column\n",
    "# with number of days since reservation.\n",
    "# until then, dropping reservation_status_date\n",
    "df2 = df2 .drop('reservation_status_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerically encode all columns of type string\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, OneHotEncoderModel, StringIndexer\n",
    "\n",
    "# list of columns to numerically encode\n",
    "col_string=['hotel', 'meal','market_segment',\n",
    "            'distribution_channel','reserved_room_type','assigned_room_type',\n",
    "            'deposit_type','customer_type','reservation_status']\n",
    "col_stringwmonth = col_string+['arrival_date_month']\n",
    "\n",
    "\n",
    "# col_num = list of new column names being changed into numeric\n",
    "col_num=[x+\"_NUMERIC\" for x in col_string]\n",
    "# don't need to change dates into numeric col_num=col_num+['arrival_date_year','arrival_date_day_of_month']\n",
    "\n",
    "# col_oh = list of columns being one-hot encoded\n",
    "col_oh=[x+\"_oh\" for x in col_string]\n",
    "# dont need to ohencode dates col_oh=col_oh+['arrival_date_year_oh','arrival_date_day_of_month_oh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_NUMERIC\").fit(df2) for column in col_stringwmonth]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_indexed = pipeline.fit(df2).transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode all columns in col_num\n",
    "ohe = OneHotEncoder(dropLast=False)\n",
    "ohe.setInputCols(col_num)\n",
    "ohe.setOutputCols(col_oh)\n",
    "model = ohe.fit(df_indexed)\n",
    "\n",
    "df_casted=model.transform(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original, non-ohencoded variables\n",
    "df_encoded=df_casted.drop(*col_stringwmonth)\n",
    "df_encoded=df_encoded.drop(*col_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(is_canceled=1, lead_time=85, arrival_date_year=2015, arrival_date_day_of_month=1, stays_in_weekend_nights=0, stays_in_week_nights=3, adults=2, children=0, babies=0, is_repeated_guest=0, previous_cancellations=0, previous_bookings_not_canceled=0, booking_changes=0, days_in_waiting_list=0, adr=82.0, required_car_parking_spaces=0, total_of_special_requests=1, arrival_date_month_NUMERIC=1.0, distribution_channel_oh=SparseVector(5, {0: 1.0}), customer_type_oh=SparseVector(4, {0: 1.0}), reservation_status_oh=SparseVector(3, {1: 1.0}), market_segment_oh=SparseVector(8, {0: 1.0}), reserved_room_type_oh=SparseVector(10, {0: 1.0}), assigned_room_type_oh=SparseVector(12, {0: 1.0}), meal_oh=SparseVector(5, {0: 1.0}), hotel_oh=SparseVector(2, {1: 1.0}), deposit_type_oh=SparseVector(3, {0: 1.0}))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- is_canceled: integer (nullable = true)\n",
      " |-- lead_time: integer (nullable = true)\n",
      " |-- arrival_date_year: integer (nullable = true)\n",
      " |-- arrival_date_day_of_month: integer (nullable = true)\n",
      " |-- stays_in_weekend_nights: integer (nullable = true)\n",
      " |-- stays_in_week_nights: integer (nullable = true)\n",
      " |-- adults: integer (nullable = true)\n",
      " |-- children: integer (nullable = true)\n",
      " |-- babies: integer (nullable = true)\n",
      " |-- is_repeated_guest: integer (nullable = true)\n",
      " |-- previous_cancellations: integer (nullable = true)\n",
      " |-- previous_bookings_not_canceled: integer (nullable = true)\n",
      " |-- booking_changes: integer (nullable = true)\n",
      " |-- days_in_waiting_list: integer (nullable = true)\n",
      " |-- adr: double (nullable = true)\n",
      " |-- required_car_parking_spaces: integer (nullable = true)\n",
      " |-- total_of_special_requests: integer (nullable = true)\n",
      " |-- arrival_date_month_NUMERIC: double (nullable = false)\n",
      " |-- distribution_channel_oh: vector (nullable = true)\n",
      " |-- customer_type_oh: vector (nullable = true)\n",
      " |-- reservation_status_oh: vector (nullable = true)\n",
      " |-- market_segment_oh: vector (nullable = true)\n",
      " |-- reserved_room_type_oh: vector (nullable = true)\n",
      " |-- assigned_room_type_oh: vector (nullable = true)\n",
      " |-- meal_oh: vector (nullable = true)\n",
      " |-- hotel_oh: vector (nullable = true)\n",
      " |-- deposit_type_oh: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_encoded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----------------+-------------------------+\n",
      "|is_canceled|lead_time|arrival_date_year|arrival_date_day_of_month|\n",
      "+-----------+---------+-----------------+-------------------------+\n",
      "|          1|       85|             2015|                        1|\n",
      "|          1|       75|             2015|                        1|\n",
      "+-----------+---------+-----------------+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_encoded.select(['is_canceled', 'lead_time','arrival_date_year','arrival_date_day_of_month']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed normalization step for now\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# norm_to_columns = ['lead_time', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children', 'babies', \n",
    "#                 'previous_cancellations', 'previous_bookings_not_canceled', 'booking_changes', 'adr', \n",
    "#                 'required_car_parking_spaces', 'total_of_special_requests']\n",
    "\n",
    "# # combine all to-norm columns into one vector named \"norm_features\"\n",
    "# assembler = VectorAssembler(inputCols=norm_to_columns, outputCol=\"norm_features\")\n",
    "# transformed = assembler.transform(df_encoded) \n",
    "# transformed = transformed.drop(*norm_to_columns)\n",
    "\n",
    "# # in the end, new column named normFeatures combined all features that are normalized\n",
    "# normalizer = Normalizer(inputCol=\"norm_features\", outputCol=\"normFeatures\", p=1.0)\n",
    "# l1NormData = normalizer.transform(transformed)\n",
    "# l1NormData = l1NormData.drop(*norm_to_columns + ['norm_features'])\n",
    "# print(\"Normalized using L^1 norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all feature columns (non-label columns) into one vector named \"feature\"\n",
    "assembler = VectorAssembler(inputCols=df_encoded.columns[1:], outputCol=\"features\")\n",
    "all_transformed = assembler.transform(df_encoded)\n",
    "\n",
    "# convert to rdd with 2 columns, label and features, where features is a DenseVector combination of all other features\n",
    "dataRdd = all_transformed.select(col(\"is_canceled\").alias(\"label\"), col(\"features\")).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  SparseVector(69, {0: 85.0, 1: 2015.0, 2: 1.0, 4: 3.0, 5: 2.0, 13: 82.0, 15: 1.0, 16: 1.0, 17: 1.0, 22: 1.0, 27: 1.0, 29: 1.0, 37: 1.0, 47: 1.0, 59: 1.0, 65: 1.0, 66: 1.0}))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [85.0,2015.0,1.0,0.0,3.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,82.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [75.0,2015.0,1.0,0.0,3.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,105.5,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# map features as floats, then input all into a DenseVector, as well as mapping labels as floats\n",
    "# then map into LabeledPoint object\n",
    "lp = dataRdd.map(lambda row: (float(row[0]), Vectors.dense([float(c) for c in row[1]])))\\\n",
    "            .map(lambda row: LabeledPoint(row[0], row[1]))\n",
    "lp.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/30 test train split\n",
    "seed = 42\n",
    "train, test = lp.randomSplit([0.7, 0.3], seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [85.0,2015.0,1.0,0.0,3.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,82.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [75.0,2015.0,1.0,0.0,3.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,105.5,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [47.0,2015.0,2.0,2.0,5.0,2.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,153.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [71.0,2015.0,3.0,0.0,2.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,110.3,0.0,2.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these variables in place of test/train.count() bc those methods take a long time, instead just call once\n",
    "test_count = test.count()\n",
    "train_count = train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 62007\n",
      "Test Dataset Count: 26513\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataset Count: \" + str(train_count))\n",
    "print(\"Test Dataset Count: \" + str(test_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# train logistic regression model with training data\n",
    "model = LogisticRegressionWithLBFGS.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (train): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on training data\n",
    "labelsAndPreds_tr = train.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy_tr = 1.0 * labelsAndPreds_tr.filter(lambda pl: pl[0] == pl[1]).count() / train_count\n",
    "print('model accuracy (train): {}'.format(accuracy_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on test data\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /home/jovyan/assignments/facebook.ipynb to pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 53326)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.7/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.7/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.7/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 73122 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 71713 bytes to /home/jovyan/assignments/facebook.pdf\n",
      "[NbConvertApp] Converting notebook /home/jovyan/assignments/hotel_booking.ipynb to pdf\n",
      "[NbConvertApp] Support files will be in hotel_booking_files/\n",
      "[NbConvertApp] Making directory ./hotel_booking_files\n",
      "[NbConvertApp] Making directory ./hotel_booking_files\n",
      "[NbConvertApp] Making directory ./hotel_booking_files\n",
      "[NbConvertApp] Making directory ./hotel_booking_files\n",
      "[NbConvertApp] Writing 127242 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 368945 bytes to /home/jovyan/assignments/hotel_booking.pdf\n",
      "[NbConvertApp] Converting notebook /home/jovyan/assignments/hotel_booking_SY-EDA.ipynb to pdf\n",
      "[NbConvertApp] Support files will be in hotel_booking_SY-EDA_files/\n",
      "[NbConvertApp] Making directory ./hotel_booking_SY-EDA_files\n",
      "[NbConvertApp] Making directory ./hotel_booking_SY-EDA_files\n",
      "[NbConvertApp] Making directory ./hotel_booking_SY-EDA_files\n",
      "[NbConvertApp] Writing 119280 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 139312 bytes to /home/jovyan/assignments/hotel_booking_SY-EDA.pdf\n",
      "[NbConvertApp] Converting notebook /home/jovyan/assignments/modeling-working.ipynb to pdf\n",
      "[NbConvertApp] Writing 70768 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 62116 bytes to /home/jovyan/assignments/modeling-working.pdf\n",
      "[NbConvertApp] Converting notebook /home/jovyan/assignments/modeling.ipynb to pdf\n",
      "[NbConvertApp] Writing 56514 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 50592 bytes to /home/jovyan/assignments/modeling.pdf\n",
      "[NbConvertApp] Converting notebook /home/jovyan/assignments/modeling_kmeans.ipynb to pdf\n",
      "[NbConvertApp] Writing 45489 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 45587 bytes to /home/jovyan/assignments/modeling_kmeans.pdf\n"
     ]
    }
   ],
   "source": [
    "# Save notebook as PDF document\n",
    "!jupyter nbconvert --to pdf `pwd`/modeling.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
