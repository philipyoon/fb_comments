{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build spark session and spark context\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"hotel\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df = spark.read.csv('hotel_bookings.csv',  inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for defining code below\n",
    "target = 'is_canceled'\n",
    "cancel_label = 1\n",
    "noncancel_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# replace the strings \"NULL\" and \"NA\" with null value\n",
    "df_withNull = df.replace('NULL', None).replace('NA', None)\n",
    "\n",
    "# replace null values in 'children' to 0 since there are only 4\n",
    "df2 = df_withNull.fillna({'children':0})\n",
    "\n",
    "# replace 'children' datatype to int\n",
    "df2 = df2.withColumn('children', col('children').cast(\"Int\"))\n",
    "df2 = df2.withColumn('arrival_date_year', col('arrival_date_year').cast(\"Int\")-2015)\n",
    "\n",
    "# drop 'company' and 'agent' due to high null count\n",
    "df2 = df2.drop('agent', 'company','country', 'arrival_date_week_number', 'reservation_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addressing reservation_status_date, which gives the date at which the last \n",
    "# reservation status was set. I'm transforming it into number of days since reservation\n",
    "# status was set, so day of arrival - reservation_status_date\n",
    "# from pyspark.sql.types import DateType\n",
    "\n",
    "# convert reservation_status_date into datetype dtype\n",
    "# temp = df2.withColumn(\"reservation_status_date\", df2[\"reservation_status_date\"].cast(DateType()))\n",
    "\n",
    "# need to combine arrival_date_year, arrival_date_month, and arrival_date_day_of_month\n",
    "# into one column and cast it to DateType, then replacing reservation_status_date column\n",
    "# with number of days since reservation.\n",
    "# until then, dropping reservation_status_date\n",
    "df2 = df2 .drop('reservation_status_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerically encode all columns of type string\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, OneHotEncoderModel, StringIndexer\n",
    "\n",
    "# list of columns to numerically encode\n",
    "col_string=['hotel', 'meal','market_segment',\n",
    "            'distribution_channel','reserved_room_type','assigned_room_type',\n",
    "            'deposit_type','customer_type','arrival_date_month']\n",
    "\n",
    "# col_num = list of new column names being changed into numeric\n",
    "col_num=[x+\"_NUMERIC\" for x in col_string]+['arrival_date_year']\n",
    "# don't need to change dates into numeric col_num=col_num+['arrival_date_year','arrival_date_day_of_month']\n",
    "\n",
    "# col_oh = list of columns being one-hot encoded\n",
    "col_oh=[x+\"_oh\" for x in col_string]+['arrival_date_year_oh']\n",
    "# dont need to ohencode dates col_oh=col_oh+['arrival_date_year_oh','arrival_date_day_of_month_oh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_NUMERIC\").fit(df2) for column in col_string]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_indexed = pipeline.fit(df2).transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode all columns in col_num\n",
    "ohe = OneHotEncoder(dropLast=False)\n",
    "ohe.setInputCols(col_num)\n",
    "ohe.setOutputCols(col_oh)\n",
    "model = ohe.fit(df_indexed)\n",
    "\n",
    "df_casted=model.transform(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original, non-ohencoded variables\n",
    "df_encoded=df_casted.drop(*col_string)\n",
    "df_encoded=df_encoded.drop(*col_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n"
     ]
    }
   ],
   "source": [
    "#removed normalization step for now\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "norm_to_columns = ['lead_time', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children', 'babies', \n",
    "                'previous_cancellations', 'previous_bookings_not_canceled', 'booking_changes', 'adr', \n",
    "                'required_car_parking_spaces', 'total_of_special_requests']\n",
    "\n",
    "# combine all to-norm columns into one vector named \"norm_features\"\n",
    "assembler = VectorAssembler(inputCols=norm_to_columns, outputCol=\"norm_features\")\n",
    "transformed = assembler.transform(df_encoded) \n",
    "transformed = transformed.drop(*norm_to_columns)\n",
    "\n",
    "# in the end, new column named normFeatures combined all features that are normalized\n",
    "normalizer = Normalizer(inputCol=\"norm_features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(transformed)\n",
    "l1NormData = l1NormData.drop(*norm_to_columns + ['norm_features'])\n",
    "print(\"Normalized using L^1 norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all feature columns (non-label columns) into one vector named \"feature\"\n",
    "assembler = VectorAssembler(inputCols=df_encoded.columns[1:], outputCol=\"features\")\n",
    "all_transformed = assembler.transform(df_encoded)\n",
    "\n",
    "# convert to rdd with 2 columns, label and features, where features is a DenseVector combination of all other features\n",
    "dataRdd = all_transformed.select(col(\"is_canceled\").alias(\"label\"), col(\"features\")).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [342.0,1.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,3.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# map features as floats, then input all into a DenseVector, as well as mapping labels as floats\n",
    "# then map into LabeledPoint object\n",
    "lp = dataRdd.map(lambda row: (float(row[0]), Vectors.dense([float(c) for c in row[1]])))\\\n",
    "            .map(lambda row: LabeledPoint(row[0], row[1]))\n",
    "lp.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/30 test train split\n",
    "seed = 314\n",
    "train, test = lp.randomSplit([0.7, 0.3], seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these variables in place of test/train.count() bc those methods take a long time, instead just call once\n",
    "test_count = test.count()\n",
    "train_count = train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# train logistic regression model with training data\n",
    "model = LogisticRegressionWithLBFGS.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.812790893287725\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on test data\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, float(model.predict(p.features))))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic with intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_int = LogisticRegressionWithLBFGS.train(train, intercept=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.8132114618964841\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on test data\n",
    "labelsAndPreds_te_int = test.map(lambda p: (p.label, model_lr_int.predict(p.features)))\n",
    "accuracy_te_int = 1.0 * labelsAndPreds_te_int.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic with interations=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_it10 = LogisticRegressionWithLBFGS.train(train, iterations=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.802557057141255\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on test data\n",
    "labelsAndPreds_te_it10 = test.map(lambda p: (p.label, model_lr_it10.predict(p.features)))\n",
    "accuracy_te_it10 = 1.0 * labelsAndPreds_te_it10.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te_it10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic with intercept and interactions=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_it10_int = LogisticRegressionWithLBFGS.train(train, iterations=10, intercept=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.8009588964279706\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on test data\n",
    "labelsAndPreds_te_it10_int = test.map(lambda p: (p.label, model_lr_it10_int.predict(p.features)))\n",
    "accuracy_te_it10_int = 1.0 * labelsAndPreds_te_it10_int.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te_it10_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[20793.  5024.]\n",
      " [ 1653.  8196.]]\n",
      "False negative rate 16.78%\n",
      "False positive rate 19.46%\n",
      "Precision(TP/all positives) 62.00%\n",
      "Recall(TP/all positive ground truth) 83.22%\n",
      "Area under PR for Linear regression = 59.11%\n",
      "Area under ROC for Linear regression = 81.88%\n"
     ]
    }
   ],
   "source": [
    "metrix1 = MulticlassMetrics(labelsAndPreds_te)\n",
    "con_metrix1=metrix1.confusionMatrix().toArray()\n",
    "\n",
    "print('Confusion matrix')\n",
    "print(con_metrix1)\n",
    "\n",
    "FN1=metrix1.falsePositiveRate(0.0)\n",
    "FP1=metrix1.falsePositiveRate(1.0)\n",
    "\n",
    "print('False negative rate {:2.2%}'.format(FN1))\n",
    "print('False positive rate {:2.2%}'.format(FP1))\n",
    "\n",
    "precision1=metrix1.precision(1.0)\n",
    "recall1=metrix1.recall(1.0)\n",
    "\n",
    "print('Precision(TP/all positives) {:2.2%}'.format(precision1))\n",
    "print('Recall(TP/all positive ground truth) {:0.2%}'.format(recall1))\n",
    "\n",
    "\n",
    "metrics_tr1 = BinaryClassificationMetrics(labelsAndPreds_te)\n",
    "\n",
    "print(\"Area under PR for Linear regression = {:2.2%}\".format(metrics_tr1.areaUnderPR))\n",
    "\n",
    "# Area under ROC curve for training data\n",
    "print(\"Area under ROC for Linear regression = {:2.2%}\".format(metrics_tr1.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "model_dt1 = DecisionTree.trainClassifier(train_sparse, 2, {})\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model_dt1.predict(test_sparse.map(lambda x: x.features))\n",
    "labelsAndPreds_te2_1 = test_sparse.map(lambda lp: lp.label).zip(predictions)\n",
    "# labelsAndPreds_te3 = test_sparse.map(lambda p: (p.label, float(model_rf.predict(p.features))))\n",
    "\n",
    "\n",
    "accuracy_te2 = 1.0 * labelsAndPreds_te2.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to LibSVM file\n",
    "MLUtils.saveAsLibSVMFile(lp, \"libsvm.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, (79,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78],[290.0,3.0,1.0,1.0,2.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,62.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, 'libsvm.txt')\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/30 test train split for libsvm data\n",
    "seed = 314\n",
    "train_sparse, test_sparse = data.randomSplit([0.7, 0.3], seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model_rf1 = RandomForest.trainClassifier(train_sparse, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model_rf1.predict(test_sparse.map(lambda x: x.features))\n",
    "labelsAndPreds_te2_1 = test_sparse.map(lambda lp: lp.label).zip(predictions)\n",
    "# labelsAndPreds_te3 = test_sparse.map(lambda p: (p.label, float(model_rf.predict(p.features))))\n",
    "\n",
    "\n",
    "accuracy_te2 = 1.0 * labelsAndPreds_te2.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.7590983009028206\n"
     ]
    }
   ],
   "source": [
    "model_rf1 = RandomForest.trainClassifier(train_sparse, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=5, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model_rf1.predict(test_sparse.map(lambda x: x.features))\n",
    "labelsAndPreds_te2 = test_sparse.map(lambda lp: lp.label).zip(predictions)\n",
    "# labelsAndPreds_te3 = test_sparse.map(lambda p: (p.label, float(model_rf.predict(p.features))))\n",
    "\n",
    "\n",
    "accuracy_te2 = 1.0 * labelsAndPreds_te2.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.768883530533281\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf1 = RandomForest.trainClassifier(train_sparse, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=10, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model_rf1.predict(test_sparse.map(lambda x: x.features))\n",
    "labelsAndPreds_te2 = test_sparse.map(lambda lp: lp.label).zip(predictions)\n",
    "# labelsAndPreds_te3 = test_sparse.map(lambda p: (p.label, float(model_rf.predict(p.features))))\n",
    "\n",
    "\n",
    "accuracy_te2 = 1.0 * labelsAndPreds_te2.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[2.2557e+04 8.3240e+03]\n",
      " [1.2000e+01 4.8660e+03]]\n",
      "False negative rate 16.78%\n",
      "False positive rate 19.46%\n",
      "Precision(TP/all positives) 36.89%\n",
      "Recall(TP/all positive ground truth) 99.75%\n",
      "Area under PR for Linear regression = 36.86%\n",
      "Area under ROC for Linear regression = 86.40%\n"
     ]
    }
   ],
   "source": [
    "metrix2 = MulticlassMetrics(labelsAndPreds_te2)\n",
    "con_metrix2=metrix2.confusionMatrix().toArray()\n",
    "\n",
    "print('Confusion matrix')\n",
    "print(con_metrix2)\n",
    "\n",
    "FN2=metrix1.falsePositiveRate(0.0)\n",
    "FP2=metrix1.falsePositiveRate(1.0)\n",
    "\n",
    "print('False negative rate {:2.2%}'.format(FN2))\n",
    "print('False positive rate {:2.2%}'.format(FP2))\n",
    "\n",
    "precision2=metrix2.precision(1.0)\n",
    "recall2=metrix2.recall(1.0)\n",
    "\n",
    "print('Precision(TP/all positives) {:2.2%}'.format(precision2))\n",
    "print('Recall(TP/all positive ground truth) {:0.2%}'.format(recall2))\n",
    "\n",
    "\n",
    "metrics_tr2 = BinaryClassificationMetrics(labelsAndPreds_te2)\n",
    "\n",
    "print(\"Area under PR for Linear regression = {:2.2%}\".format(metrics_tr2.areaUnderPR))\n",
    "\n",
    "# Area under ROC curve for training data\n",
    "print(\"Area under ROC for Linear regression = {:2.2%}\".format(metrics_tr2.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.5877586496943868\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "model_svm_1 = SVMWithSGD.train(train,iterations=10)\n",
    "\n",
    "labelsAndPreds_te3_1 = test.map(lambda p: (p.label, float(model_svm_1.predict(p.features))))\n",
    "accuracy_te3 = 1.0 * labelsAndPreds_te3_1.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[11345.  3602.]\n",
      " [11101.  9618.]]\n",
      "False negative rate 16.78%\n",
      "False positive rate 19.46%\n",
      "Precision(TP/all positives) 72.75%\n",
      "Recall(TP/all positive ground truth) 46.42%\n",
      "Area under PR for Linear regression = 36.86%\n",
      "Area under ROC for Linear regression = 86.40%\n"
     ]
    }
   ],
   "source": [
    "metrix2 = MulticlassMetrics(labelsAndPreds_te3_1)\n",
    "con_metrix2=metrix2.confusionMatrix().toArray()\n",
    "\n",
    "print('Confusion matrix')\n",
    "print(con_metrix2)\n",
    "\n",
    "FN2=metrix1.falsePositiveRate(0.0)\n",
    "FP2=metrix1.falsePositiveRate(1.0)\n",
    "\n",
    "print('False negative rate {:2.2%}'.format(FN2))\n",
    "print('False positive rate {:2.2%}'.format(FP2))\n",
    "\n",
    "precision2=metrix2.precision(1.0)\n",
    "recall2=metrix2.recall(1.0)\n",
    "\n",
    "print('Precision(TP/all positives) {:2.2%}'.format(precision2))\n",
    "print('Recall(TP/all positive ground truth) {:0.2%}'.format(recall2))\n",
    "\n",
    "\n",
    "metrics_tr2 = BinaryClassificationMetrics(labelsAndPreds_te2)\n",
    "\n",
    "print(\"Area under PR for Linear regression = {:2.2%}\".format(metrics_tr2.areaUnderPR))\n",
    "\n",
    "# Area under ROC curve for training data\n",
    "print(\"Area under ROC for Linear regression = {:2.2%}\".format(metrics_tr2.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.6293388661470308\n"
     ]
    }
   ],
   "source": [
    "model_svm_2 = SVMWithSGD.train(train,iterations=10,intercept=True)\n",
    "\n",
    "labelsAndPreds_te3_2 = test.map(lambda p: (p.label, float(model_svm_2.predict(p.features))))\n",
    "accuracy_te3_2 = 1.0 * labelsAndPreds_te3_2.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te3_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.6506476756574889\n"
     ]
    }
   ],
   "source": [
    "model_svm_3 = SVMWithSGD.train(train,iterations=20,intercept=True)\n",
    "\n",
    "labelsAndPreds_te3_3 = test.map(lambda p: (p.label, float(model_svm_3.predict(p.features))))\n",
    "accuracy_te3_3 = 1.0 * labelsAndPreds_te3_3.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te3_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.6283014635787585\n"
     ]
    }
   ],
   "source": [
    "model_svm_4 = SVMWithSGD.train(train,iterations=30,intercept=True, regType='l1')\n",
    "\n",
    "labelsAndPreds_te3_4 = test.map(lambda p: (p.label, float(model_svm_4.predict(p.features))))\n",
    "accuracy_te3_4 = 1.0 * labelsAndPreds_te3_4.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te3_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[21906. 11920.]\n",
      " [  540.  1300.]]\n",
      "False negative rate 29.35%\n",
      "False positive rate 35.24%\n",
      "Precision(TP/all positives) 9.83%\n",
      "Recall(TP/all positive ground truth) 70.65%\n",
      "Area under PR for Linear regression = 9.15%\n",
      "Area under ROC for Linear regression = 67.71%\n"
     ]
    }
   ],
   "source": [
    "metrix3 = MulticlassMetrics(labelsAndPreds_te3_3)\n",
    "con_metrix3=metrix3.confusionMatrix().toArray()\n",
    "\n",
    "print('Confusion matrix')\n",
    "print(con_metrix3)\n",
    "\n",
    "FN3=metrix3.falsePositiveRate(0.0)\n",
    "FP3=metrix3.falsePositiveRate(1.0)\n",
    "\n",
    "print('False negative rate {:2.2%}'.format(FN3))\n",
    "print('False positive rate {:2.2%}'.format(FP3))\n",
    "\n",
    "precision3=metrix3.precision(1.0)\n",
    "recall3=metrix3.recall(1.0)\n",
    "\n",
    "print('Precision(TP/all positives) {:2.2%}'.format(precision3))\n",
    "print('Recall(TP/all positive ground truth) {:0.2%}'.format(recall3))\n",
    "\n",
    "metrics_tr3 = BinaryClassificationMetrics(labelsAndPreds_te3)\n",
    "\n",
    "print(\"Area under PR for Linear regression = {:2.2%}\".format(metrics_tr3.areaUnderPR))\n",
    "\n",
    "# Area under ROC curve for training data\n",
    "print(\"Area under ROC for Linear regression = {:2.2%}\".format(metrics_tr3.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.7590983009028206\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTreeModel' object has no attribute 'numTrees'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-9dd1a6e13d69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_dt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumTrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DecisionTreeModel' object has no attribute 'numTrees'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
