{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build spark session and spark context\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"hotel\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df = spark.read.csv('hotel_bookings.csv',  inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql.functions import col \n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for defining code below\n",
    "target = 'is_canceled'\n",
    "cancel_label = 1\n",
    "noncancel_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balancing a DataFrame with Downsampling\n",
    "\n",
    "def downsample(df, target, cancel_label, noncancel_label):\n",
    "    \"\"\"\n",
    "    df               spark dataframe\n",
    "    target           str, target variable\n",
    "    cancel_label     int, value of canceled booking\n",
    "    noncancel_label  int, value of non-canceled booking\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ### ENTER CODE HERE\n",
    "    \n",
    "    from pyspark.sql.functions import col\n",
    "    \n",
    "    #count of canceled and non-canceled labels\n",
    "    cancel_n = df.filter(col(target) == cancel_label).count()\n",
    "    noncancel_n = df.filter(col(target) == noncancel_label).count()\n",
    "    \n",
    "    #df split by having either the poitive or negative labels\n",
    "    df_cancel = df.filter(col(target) == cancel_label)\n",
    "    df_noncancel = df.filter(col(target) == noncancel_label)\n",
    "    \n",
    "    \n",
    "    if cancel_n > noncancel_n:\n",
    "        #amount to sample from is fraction of low noncancel/full cancel\n",
    "        df_a = df_cancel.sample(fraction = (noncancel_n/cancel_n))\n",
    "        #combine df_cancel sample with full df_noncancel\n",
    "        df_b = df_noncancel.union(df_a)\n",
    "    elif noncancel_n > cancel_n:\n",
    "        #amount to sample from is fraction of low cancel/full non-cancel\n",
    "        df_a = df_noncancel.sample(fraction = (cancel_n/noncancel_n))\n",
    "        #combine df_noncancel sample with full df_cancel\n",
    "        df_b = df_cancel.union(df_a)\n",
    "    else:\n",
    "        #if count of df_cancel = df_noncancel, then just use original df\n",
    "        df_b = df\n",
    "\n",
    "    return df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|is_canceled|count|\n",
      "+-----------+-----+\n",
      "|          1|44224|\n",
      "|          0|44125|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call your downsample function here, and show the count by label\n",
    "df_downsample = downsample(df, target, cancel_label, noncancel_label)\n",
    "df_downsample.groupBy(target).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# replace the strings \"NULL\" and \"NA\" with null value\n",
    "df_withNull = df_downsample.replace('NULL', None).replace('NA', None)\n",
    "\n",
    "# replace null values in 'children' to 0 since there are only 4\n",
    "df2 = df_withNull.fillna({'children':0})\n",
    "\n",
    "# replace 'children' datatype to int\n",
    "df2 = df2.withColumn('children', col('children').cast(\"Int\"))\n",
    "\n",
    "# drop 'company' and 'agent' due to high null count\n",
    "df2 = df2.drop('agent', 'company','country', 'arrival_date_week_number', 'reservation_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addressing reservation_status_date, which gives the date at which the last \n",
    "# reservation status was set. I'm transforming it into number of days since reservation\n",
    "# status was set, so day of arrival - reservation_status_date\n",
    "# from pyspark.sql.types import DateType\n",
    "\n",
    "# convert reservation_status_date into datetype dtype\n",
    "# temp = df2.withColumn(\"reservation_status_date\", df2[\"reservation_status_date\"].cast(DateType()))\n",
    "\n",
    "# need to combine arrival_date_year, arrival_date_month, and arrival_date_day_of_month\n",
    "# into one column and cast it to DateType, then replacing reservation_status_date column\n",
    "# with number of days since reservation.\n",
    "# until then, dropping reservation_status_date\n",
    "df2 = df2 .drop('reservation_status_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerically encode all columns of type string\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, OneHotEncoderModel, StringIndexer\n",
    "\n",
    "# list of columns to numerically encode\n",
    "col_string=['hotel', 'meal','market_segment',\n",
    "            'distribution_channel','reserved_room_type','assigned_room_type',\n",
    "            'deposit_type','customer_type']\n",
    "col_stringwmonth = col_string+['arrival_date_month']\n",
    "\n",
    "\n",
    "# col_num = list of new column names being changed into numeric\n",
    "col_num=[x+\"_NUMERIC\" for x in col_string]\n",
    "# don't need to change dates into numeric col_num=col_num+['arrival_date_year','arrival_date_day_of_month']\n",
    "\n",
    "# col_oh = list of columns being one-hot encoded\n",
    "col_oh=[x+\"_oh\" for x in col_string]\n",
    "# dont need to ohencode dates col_oh=col_oh+['arrival_date_year_oh','arrival_date_day_of_month_oh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_NUMERIC\").fit(df2) for column in col_stringwmonth]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_indexed = pipeline.fit(df2).transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode all columns in col_num\n",
    "ohe = OneHotEncoder(dropLast=False)\n",
    "ohe.setInputCols(col_num)\n",
    "ohe.setOutputCols(col_oh)\n",
    "model = ohe.fit(df_indexed)\n",
    "\n",
    "df_casted=model.transform(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original, non-ohencoded variables\n",
    "df_encoded=df_casted.drop(*col_stringwmonth)\n",
    "df_encoded=df_encoded.drop(*col_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n"
     ]
    }
   ],
   "source": [
    "#removed normalization step for now\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "norm_to_columns = ['lead_time', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children', 'babies', \n",
    "                'previous_cancellations', 'previous_bookings_not_canceled', 'booking_changes', 'adr', \n",
    "                'required_car_parking_spaces', 'total_of_special_requests']\n",
    "\n",
    "# combine all to-norm columns into one vector named \"norm_features\"\n",
    "assembler = VectorAssembler(inputCols=norm_to_columns, outputCol=\"norm_features\")\n",
    "transformed = assembler.transform(df_encoded) \n",
    "transformed = transformed.drop(*norm_to_columns)\n",
    "\n",
    "# in the end, new column named normFeatures combined all features that are normalized\n",
    "normalizer = Normalizer(inputCol=\"norm_features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(transformed)\n",
    "l1NormData = l1NormData.drop(*norm_to_columns + ['norm_features'])\n",
    "print(\"Normalized using L^1 norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all feature columns (non-label columns) into one vector named \"feature\"\n",
    "assembler = VectorAssembler(inputCols=df_encoded.columns[1:], outputCol=\"features\")\n",
    "all_transformed = assembler.transform(df_encoded)\n",
    "\n",
    "# convert to rdd with 2 columns, label and features, where features is a DenseVector combination of all other features\n",
    "dataRdd = all_transformed.select(col(\"is_canceled\").alias(\"label\"), col(\"features\")).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [85.0,2015.0,1.0,0.0,3.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,82.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# map features as floats, then input all into a DenseVector, as well as mapping labels as floats\n",
    "# then map into LabeledPoint object\n",
    "lp = dataRdd.map(lambda row: (float(row[0]), Vectors.dense([float(c) for c in row[1]])))\\\n",
    "            .map(lambda row: LabeledPoint(row[0], row[1]))\n",
    "lp.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/30 test train split\n",
    "seed = 314\n",
    "train, test = lp.randomSplit([0.7, 0.3], seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these variables in place of test/train.count() bc those methods take a long time, instead just call once\n",
    "test_count = test.count()\n",
    "train_count = train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# train logistic regression model with training data\n",
    "model = LogisticRegressionWithLBFGS.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (train): 0.7672086063253256\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on training data\n",
    "labelsAndPreds_tr = train.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy_tr = 1.0 * labelsAndPreds_tr.filter(lambda pl: pl[0] == pl[1]).count() / train_count\n",
    "print('model accuracy (train): {}'.format(accuracy_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.7655460918067385\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on test data\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test_count\n",
    "print('model accuracy (test): {}'.format(accuracy_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Model Evaluation\n",
    "\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "predictionAndLabels_te = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics_te = BinaryClassificationMetrics(predictionAndLabels_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR for testing = 0.7402845326306933\n",
      "Area under ROC for testing = 0.7654418356937008\n"
     ]
    }
   ],
   "source": [
    "# Area under precision-recall curve for test data\n",
    "print(\"Area under PR for testing = %s\" % metrics_te.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve for test data\n",
    "print(\"Area under ROC for testing = %s\" % metrics_te.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10496.  2834.]\n",
      " [ 3387.  9817.]]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the confusion matrix for test data\n",
    "metrics_matrix_te = MulticlassMetrics(predictionAndLabels_te)\n",
    "labelsAndPreds_te.take(3)\n",
    "print(\"Confusion Matrix:\\n{}\".format(metrics_matrix_te.confusionMatrix().toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Compute raw scores on the training set\n",
    "predictionAndLabels_tr = train.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics_tr = BinaryClassificationMetrics(predictionAndLabels_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR for training set = 0.7455078105853685\n",
      "Area under ROC for training set = 0.7672906689680274\n"
     ]
    }
   ],
   "source": [
    "# Area under precision-recall curve for training data\n",
    "print(\"Area under PR for training set = %s\" % metrics_tr.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve for training data\n",
    "print(\"Area under ROC for training set = %s\" % metrics_tr.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[24323.  6472.]\n",
      " [ 7918. 23102.]]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the confusion matrix for training data\n",
    "metrics_matrix_tr = MulticlassMetrics(predictionAndLabels_tr)\n",
    "labelsAndPreds_tr.take(3)\n",
    "print(\"Confusion Matrix:\\n{}\".format(metrics_matrix_tr.confusionMatrix().toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
